In this paper, we present different system architectures to handle Query 1 and Query 2. In case of Query 1, we need to forecast average load of each house as well as of each plug. The predicted load only depends on the values inside the house. Whereas, in case of Query 2, we need to find percentage of plugs having median larger than the global median, every time we receive a new event. The global median is computed using all the data of all the plugs in the system and then compared with median of each plug. We, therefore, distill down to following set of reasons for such a design choice-

\begin{enumerate}
\item It is possible to process data for different houses in parallel (possibly on different nodes) in case of Query 1. On the other hand, distributed system in case of Query 2, will require large number of exchange of messages among the processes before putting an output event.
\item Query 2 computes median for large set of data, potentially median of $(24 \mbox{hrs} * 3600 \mbox{sec} * 2125 \mbox{plugs}) \approx 270 \ \mbox{million}$ numbers. Calculating exact median of so many numbers multiple times every second can heavily slow down the system. We get away by computing approximate medians in such a case. Whereas in Query 1, we compute medians of at most 30 numbers (each data item corresponds to each day in the month). Computing exact median would be feasible in this case.
\end{enumerate}

We have used C++ programming language in order to implement the solution we present in this paper. We have extensively used arrays and hashmaps in order to store data. Sorted arrays allow binary search to quickly search for an element, whereas hashmaps are efficient data structures to store key value pair. Due to the fact that number of households in a house and number of plugs in a household are not known beforehand, we had to sometimes use hashmaps in place of arrays to store data containers for different plugs, households.
